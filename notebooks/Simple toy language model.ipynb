{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de770854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb486dea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our silly language\n",
    "# W 0 W W W W W\n",
    "\n",
    "def generate(L=64, max_start=10):\n",
    "    start_len = np.random.randint(2, max_start)\n",
    "    start = list(np.random.randint(1, 4, size=start_len))\n",
    "    out = start + [0]\n",
    "    while len(out) < L:\n",
    "        out += start\n",
    "    return out[:L]\n",
    "    \n",
    "generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "537ca6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape torch.Size([10, 20, 32])\n",
      "q shape torch.Size([10, 4, 20, 8])\n",
      "attn out shape torch.Size([10, 4, 20, 8])\n",
      "attn logits shapes torch.Size([10, 4, 20, 20]) torch.Size([10, 4, 20, 20])\n",
      "max diff 0.0\n",
      "attn out shapes torch.Size([10, 4, 20, 8]) torch.Size([10, 4, 20, 8])\n",
      "attn diff tensor(1.5698e-14, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 20, 32])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEBUG = True\n",
    "\n",
    "class SelfAttentionWithoutMask(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.q_proj = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.k_proj = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.v_proj = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.o_proj = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = embed_size // num_heads\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, L, E = x.shape\n",
    "        q = self.q_proj(x).reshape(B, L, self.num_heads, self.head_size).transpose(1, 2)\n",
    "        k = self.k_proj(x).reshape(B, L, self.num_heads, self.head_size).transpose(1, 2)\n",
    "        v = self.v_proj(x).reshape(B, L, self.num_heads, self.head_size).transpose(1, 2)\n",
    "        \n",
    "        if DEBUG:\n",
    "            print(\"input shape\", x.shape)\n",
    "            print(\"q shape\", q.shape)\n",
    "        \n",
    "            \n",
    "        # This is what you want to use in practice\n",
    "        attn_out = F.scaled_dot_product_attention(q, k, v)\n",
    "        if DEBUG:\n",
    "            print(\"attn out shape\", attn_out.shape)\n",
    "            \n",
    "            # Here we explicitly calculate attention\n",
    "            \n",
    "            # First we calculate attention logits, these two formulas are equivalent\n",
    "            attn_logits = torch.einsum(\"bhqe, bhke -> bhqk\", q, k) / np.sqrt(self.head_size)\n",
    "            attn_logits2 = q.matmul(k.transpose(-2,-1)) / np.sqrt(self.head_size)\n",
    "            print(\"attn logits shapes\", attn_logits.shape, attn_logits2.shape)\n",
    "            print(\"max diff\", (attn_logits - attn_logits2).abs().amax().item())\n",
    "            \n",
    "            attn_matrix = torch.softmax(attn_logits, dim=-1)\n",
    "            \n",
    "            attn_out2 = torch.einsum(\"bhqk, bhke -> bhqe\", attn_matrix, v)\n",
    "            print(\"attn out shapes\", attn_out2.shape, attn_out.shape)\n",
    "            \n",
    "            print(\"attn diff\", (attn_out - attn_out2).square().sum() / attn_out.square().sum())\n",
    "            \n",
    "            \n",
    "        reshaped = attn_out.transpose(1,2).reshape(B, L, E)\n",
    "        return self.o_proj(reshaped)\n",
    "            \n",
    "        \n",
    "            \n",
    "sa = SelfAttentionWithoutMask(32, 4)\n",
    "\n",
    "sa(torch.rand(10, 20, 32)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de0c001d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 20, 32])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will use causal attention, i.e. each sequence step sees only steps before it\n",
    "\n",
    "DEBUG = True\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.q_proj = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.k_proj = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.v_proj = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.o_proj = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = embed_size // num_heads\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, L, E = x.shape\n",
    "        q = self.q_proj(x).reshape(B, L, self.num_heads, self.head_size).transpose(1, 2)\n",
    "        k = self.k_proj(x).reshape(B, L, self.num_heads, self.head_size).transpose(1, 2)\n",
    "        v = self.v_proj(x).reshape(B, L, self.num_heads, self.head_size).transpose(1, 2)\n",
    "        \n",
    "        # This is what you want to use in practice\n",
    "        attn_out = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        reshaped = attn_out.transpose(1,2).reshape(B, L, E)\n",
    "        return self.o_proj(reshaped)\n",
    "            \n",
    "        \n",
    "            \n",
    "sa = CausalSelfAttention(32, 4)\n",
    "\n",
    "sa(torch.rand(10, 20, 32)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8c36555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0025, 0.0017, 0.0012, 0.0009, 0.0007, 0.0005,\n",
       "         0.0004]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets have some input and change 4th element\n",
    "inp = torch.rand(1, 10, 32)\n",
    "out1 = sa(inp)\n",
    "\n",
    "inp[:,3] += 0.1\n",
    "\n",
    "out2 = sa(inp)\n",
    "\n",
    "# We only see changes after 4th element, first 3 are not affected\n",
    "(out1 - out2).square().sum(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b862367c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 4])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's build the rest of the model\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        \n",
    "        self.self_attn = CausalSelfAttention(embed_size, num_heads)\n",
    "        \n",
    "        # We also have an MLP block, it's contents differ, LLaMA uses complicated structure, we will\n",
    "        # keep the simple one\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_size, 4*embed_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(4*embed_size, embed_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Why do we do self addition and normalization? Next lecture!\n",
    "        x = x + self.self_attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm1(x))\n",
    "        return x\n",
    "        \n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_blocks, embed_size, num_heads, vocab_size, max_poses):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(embed_size, num_heads) for _ in range(n_blocks)]\n",
    "        )\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(max_poses, embed_size))\n",
    "        self.out_norm = nn.LayerNorm(embed_size)\n",
    "        self.out = nn.Linear(embed_size, vocab_size)\n",
    "        \n",
    "    def forward(self, sequences):\n",
    "        embeded = self.embed(sequences)\n",
    "        #embeded = embeded + self.pos_embed[:embeded.shape[1]]\n",
    "        \n",
    "        if DEBUG:\n",
    "            print(\"embeded shape\", embeded.shape)\n",
    "            \n",
    "        embeded = self.blocks(embeded)\n",
    "        \n",
    "        embeded = self.out_norm(embeded)\n",
    "        return self.out(embeded)\n",
    "    \n",
    "model = Model(2, 32, 2, 4, 64)\n",
    "\n",
    "# For each sequence position we got probability logits (inputs for softmax) for the next token\n",
    "model(torch.randint(4, size=(2,16))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8024a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 1]\n",
      "[3, 1, 0]\n",
      "[3, 1, 0, 3]\n",
      "[3, 1, 0, 3, 1]\n",
      "[3, 1, 0, 3, 1, 0]\n",
      "[3, 1, 0, 3, 1, 0, 3]\n",
      "[3, 1, 0, 3, 1, 0, 3, 3]\n",
      "[3, 1, 0, 3, 1, 0, 3, 3, 3]\n",
      "[3, 1, 0, 3, 1, 0, 3, 3, 3, 3]\n",
      "[3, 1, 0, 3, 1, 0, 3, 3, 3, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "\n",
    "# How to sample (suboptimal implementation)\n",
    "\n",
    "seq = [3]\n",
    "for i in range(10):\n",
    "    out = model(torch.LongTensor([seq]))[0,-1]\n",
    "    # Convert to probs\n",
    "    out_p = torch.softmax(out, dim=-1)\n",
    "    # Sample next token\n",
    "    next_token = torch.multinomial(out_p, num_samples=1).item()\n",
    "    seq.append(next_token)\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08efd166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.3154503107070923 acc 0.4087301587301587 acc after step 10 0.41782407407407407\n",
      "100 0.926128625869751 acc 0.6805555555555556 acc after step 10 0.7118055555555556\n",
      "200 0.9247331023216248 acc 0.6190476190476191 acc after step 10 0.6493055555555556\n",
      "300 0.9681235551834106 acc 0.5 acc after step 10 0.5185185185185185\n",
      "400 0.9258744120597839 acc 0.5932539682539683 acc after step 10 0.6203703703703703\n",
      "500 0.8625814318656921 acc 0.6656746031746031 acc after step 10 0.6921296296296297\n",
      "600 0.9161351919174194 acc 0.5486111111111112 acc after step 10 0.5706018518518519\n",
      "700 0.8958262801170349 acc 0.6111111111111112 acc after step 10 0.6400462962962963\n",
      "800 0.9110379815101624 acc 0.5565476190476191 acc after step 10 0.5729166666666666\n",
      "900 0.9073696732521057 acc 0.6448412698412699 acc after step 10 0.6782407407407407\n",
      "1000 0.9195709228515625 acc 0.5873015873015873 acc after step 10 0.6099537037037037\n",
      "1100 0.8853381276130676 acc 0.6130952380952381 acc after step 10 0.6412037037037037\n",
      "1200 0.9110097885131836 acc 0.6001984126984127 acc after step 10 0.6203703703703703\n",
      "1300 0.9548888206481934 acc 0.5575396825396826 acc after step 10 0.5833333333333334\n",
      "1400 0.7900574207305908 acc 0.6845238095238095 acc after step 10 0.7094907407407407\n",
      "1500 0.7759568691253662 acc 0.6696428571428571 acc after step 10 0.6909722222222222\n",
      "1600 0.8849334120750427 acc 0.5952380952380952 acc after step 10 0.6122685185185185\n",
      "1700 0.7764008641242981 acc 0.6785714285714286 acc after step 10 0.7025462962962963\n",
      "1800 0.8608883619308472 acc 0.6061507936507936 acc after step 10 0.6296296296296297\n",
      "1900 0.9636802673339844 acc 0.5873015873015873 acc after step 10 0.6145833333333334\n",
      "2000 0.8194169998168945 acc 0.6567460317460317 acc after step 10 0.6759259259259259\n",
      "2100 0.8282572031021118 acc 0.6517857142857143 acc after step 10 0.6747685185185185\n",
      "2200 0.6802544593811035 acc 0.7212301587301587 acc after step 10 0.7442129629629629\n",
      "2300 0.7726014256477356 acc 0.6636904761904762 acc after step 10 0.6875\n",
      "2400 0.7712059020996094 acc 0.6339285714285714 acc after step 10 0.6597222222222222\n",
      "2500 0.7430873513221741 acc 0.6805555555555556 acc after step 10 0.7048611111111112\n",
      "2600 0.7255679965019226 acc 0.7172619047619048 acc after step 10 0.7569444444444444\n",
      "2700 0.6796920895576477 acc 0.7549603174603174 acc after step 10 0.7928240740740741\n",
      "2800 0.72151118516922 acc 0.7142857142857143 acc after step 10 0.7453703703703703\n",
      "2900 0.7543478608131409 acc 0.6636904761904762 acc after step 10 0.6863425925925926\n",
      "3000 0.6899973154067993 acc 0.7113095238095238 acc after step 10 0.7407407407407407\n",
      "3100 0.6271474361419678 acc 0.7351190476190477 acc after step 10 0.7604166666666666\n",
      "3200 0.5831529498100281 acc 0.7628968253968254 acc after step 10 0.7881944444444444\n",
      "3300 0.7552295327186584 acc 0.6597222222222222 acc after step 10 0.6909722222222222\n",
      "3400 0.740328848361969 acc 0.691468253968254 acc after step 10 0.7210648148148148\n",
      "3500 0.8352604508399963 acc 0.5972222222222222 acc after step 10 0.6296296296296297\n",
      "3600 0.6490700840950012 acc 0.7113095238095238 acc after step 10 0.7430555555555556\n",
      "3700 0.631051778793335 acc 0.7103174603174603 acc after step 10 0.7349537037037037\n",
      "3800 0.6706543564796448 acc 0.7113095238095238 acc after step 10 0.7453703703703703\n",
      "3900 0.6915107369422913 acc 0.6746031746031746 acc after step 10 0.7037037037037037\n",
      "4000 0.6243019700050354 acc 0.7380952380952381 acc after step 10 0.7650462962962963\n",
      "4100 0.6295707821846008 acc 0.7271825396825397 acc after step 10 0.7627314814814815\n",
      "4200 0.7949735522270203 acc 0.6240079365079365 acc after step 10 0.6516203703703703\n",
      "4300 0.6247406005859375 acc 0.7321428571428571 acc after step 10 0.7615740740740741\n",
      "4400 0.707908034324646 acc 0.6646825396825397 acc after step 10 0.6863425925925926\n",
      "4500 0.6247695684432983 acc 0.6954365079365079 acc after step 10 0.7291666666666666\n",
      "4600 0.5534704327583313 acc 0.7232142857142857 acc after step 10 0.7627314814814815\n",
      "4700 0.7275985479354858 acc 0.7003968253968254 acc after step 10 0.7430555555555556\n",
      "4800 0.5888355374336243 acc 0.7390873015873016 acc after step 10 0.7685185185185185\n",
      "4900 0.6800357103347778 acc 0.7132936507936508 acc after step 10 0.7442129629629629\n",
      "5000 0.5541300773620605 acc 0.7559523809523809 acc after step 10 0.7951388888888888\n",
      "5100 0.5038314461708069 acc 0.7916666666666666 acc after step 10 0.8356481481481481\n",
      "5200 0.5707560777664185 acc 0.7619047619047619 acc after step 10 0.7974537037037037\n",
      "5300 0.525083601474762 acc 0.7380952380952381 acc after step 10 0.7719907407407407\n",
      "5400 0.48142513632774353 acc 0.7896825396825397 acc after step 10 0.8229166666666666\n",
      "5500 0.5789351463317871 acc 0.7321428571428571 acc after step 10 0.7800925925925926\n",
      "5600 0.645485520362854 acc 0.7152777777777778 acc after step 10 0.7534722222222222\n",
      "5700 0.4892251789569855 acc 0.7599206349206349 acc after step 10 0.7928240740740741\n",
      "5800 0.5286424160003662 acc 0.753968253968254 acc after step 10 0.7881944444444444\n",
      "5900 0.5588637590408325 acc 0.7569444444444444 acc after step 10 0.7974537037037037\n",
      "6000 0.6470367908477783 acc 0.7033730158730159 acc after step 10 0.7442129629629629\n",
      "6100 0.4711936116218567 acc 0.8194444444444444 acc after step 10 0.8587962962962963\n",
      "6200 0.47020742297172546 acc 0.8184523809523809 acc after step 10 0.8518518518518519\n",
      "6300 0.5112242102622986 acc 0.7817460317460317 acc after step 10 0.8125\n",
      "6400 0.6202538013458252 acc 0.7113095238095238 acc after step 10 0.7546296296296297\n",
      "6500 0.5062375664710999 acc 0.7331349206349206 acc after step 10 0.7581018518518519\n",
      "6600 0.5288425087928772 acc 0.7490079365079365 acc after step 10 0.7789351851851852\n",
      "6700 0.6458727121353149 acc 0.6924603174603174 acc after step 10 0.7314814814814815\n",
      "6800 0.48395681381225586 acc 0.7797619047619048 acc after step 10 0.8206018518518519\n",
      "6900 0.5548433661460876 acc 0.7599206349206349 acc after step 10 0.8032407407407407\n",
      "7000 0.5955570936203003 acc 0.7420634920634921 acc after step 10 0.7824074074074074\n",
      "7100 0.484223872423172 acc 0.7767857142857143 acc after step 10 0.8136574074074074\n",
      "7200 0.5341112613677979 acc 0.746031746031746 acc after step 10 0.7766203703703703\n",
      "7300 0.5173537731170654 acc 0.7946428571428571 acc after step 10 0.8460648148148148\n",
      "7400 0.5099161863327026 acc 0.7767857142857143 acc after step 10 0.8194444444444444\n",
      "7500 0.5356850028038025 acc 0.7222222222222222 acc after step 10 0.7569444444444444\n",
      "7600 0.4701211154460907 acc 0.7748015873015873 acc after step 10 0.8194444444444444\n",
      "7700 0.5932937860488892 acc 0.7103174603174603 acc after step 10 0.7523148148148148\n",
      "7800 0.6007969975471497 acc 0.7162698412698413 acc after step 10 0.75\n",
      "7900 0.5045475959777832 acc 0.7718253968253969 acc after step 10 0.8113425925925926\n",
      "8000 0.4317657947540283 acc 0.8204365079365079 acc after step 10 0.8599537037037037\n",
      "8100 0.4827585518360138 acc 0.8015873015873016 acc after step 10 0.8368055555555556\n",
      "8200 0.5614928603172302 acc 0.7341269841269841 acc after step 10 0.7858796296296297\n",
      "8300 0.4784204363822937 acc 0.7956349206349206 acc after step 10 0.8391203703703703\n",
      "8400 0.39581137895584106 acc 0.8363095238095238 acc after step 10 0.8831018518518519\n",
      "8500 0.5303964018821716 acc 0.7718253968253969 acc after step 10 0.8182870370370371\n",
      "8600 0.5607966780662537 acc 0.748015873015873 acc after step 10 0.7939814814814815\n",
      "8700 0.453731507062912 acc 0.8055555555555556 acc after step 10 0.8530092592592593\n",
      "8800 0.4265904128551483 acc 0.8075396825396826 acc after step 10 0.8472222222222222\n",
      "8900 0.47450435161590576 acc 0.7996031746031746 acc after step 10 0.8472222222222222\n",
      "9000 0.4733964800834656 acc 0.7926587301587301 acc after step 10 0.8391203703703703\n",
      "9100 0.616746723651886 acc 0.7083333333333334 acc after step 10 0.7430555555555556\n",
      "9200 0.39784157276153564 acc 0.8323412698412699 acc after step 10 0.875\n",
      "9300 0.3282455801963806 acc 0.875 acc after step 10 0.9212962962962963\n",
      "9400 0.48249551653862 acc 0.7867063492063492 acc after step 10 0.8171296296296297\n",
      "9500 0.5008528232574463 acc 0.7797619047619048 acc after step 10 0.8217592592592593\n",
      "9600 0.5828021764755249 acc 0.7242063492063492 acc after step 10 0.7685185185185185\n",
      "9700 0.49796605110168457 acc 0.7896825396825397 acc after step 10 0.8356481481481481\n",
      "9800 0.48927003145217896 acc 0.7896825396825397 acc after step 10 0.8344907407407407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9900 0.4925936460494995 acc 0.7708333333333334 acc after step 10 0.8148148148148148\n"
     ]
    }
   ],
   "source": [
    "# Let's train\n",
    "# Notice that we get train all positions at once\n",
    "# E.g.: Output from step 3 is only influence by first 3 steps and not by step 4\n",
    "\n",
    "model.cuda()\n",
    "opt = torch.optim.AdamW(model.parameters(), 1e-4, weight_decay=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for step in range(10000):\n",
    "    data = torch.LongTensor([generate() for _ in range(16)]).cuda()\n",
    "    out = model(data)\n",
    "    out_pred = out.argmax(dim=-1)\n",
    "    # Notice the shift\n",
    "    # Flatten is because of pytorch quirks\n",
    "    loss = loss_fn(out[:,:-1].flatten(0, -2), data[:,1:].flatten())\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        print(step, loss.item(), \"acc\", (out_pred[:,:-1] == data[:,1:]).sum().item() / data[:,1:].numel(),\n",
    "              \"acc after step 10\", (out_pred[:,9:-1] == data[:,10:]).sum().item() / data[:,10:].numel())\n",
    "    \n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e99fde4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 1, 2, 3, 0, 3]\n",
      "[3, 1, 2, 3, 0, 3, 2]\n",
      "[3, 1, 2, 3, 0, 3, 2, 3]\n",
      "[3, 1, 2, 3, 0, 3, 2, 3, 1]\n",
      "[3, 1, 2, 3, 0, 3, 2, 3, 1, 2]\n",
      "[3, 1, 2, 3, 0, 3, 2, 3, 1, 2, 3]\n",
      "[3, 1, 2, 3, 0, 3, 2, 3, 1, 2, 3, 3]\n",
      "[3, 1, 2, 3, 0, 3, 2, 3, 1, 2, 3, 3, 1]\n",
      "[3, 1, 2, 3, 0, 3, 2, 3, 1, 2, 3, 3, 1, 2]\n",
      "[3, 1, 2, 3, 0, 3, 2, 3, 1, 2, 3, 3, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# Let's sample again\n",
    "seq = [3, 1, 2, 3, 0]\n",
    "\n",
    "for i in range(10):\n",
    "    out = model(torch.LongTensor([seq]).cuda())[0,-1]\n",
    "    # Convert to probs\n",
    "    out_p = torch.softmax(out, dim=-1)\n",
    "    # Sample next token\n",
    "    next_token = torch.multinomial(out_p, num_samples=1).item()\n",
    "    seq.append(next_token)\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07b00a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
