[Classroom](https://classroom.google.com/c/NDY5NDE1NDkzOTI5?cjc=yr5hpez)

## Contact

`<surname>@fmph.uniba.sk`

## Grading

Three projects:
* [Autograd](https://usamec.github.io/ml2/hw1) - 20% - deadline 9th March 
* Relativelly simple implementation of something exotic. [Lottery tickets](https://usamec.github.io/ml2/hw2) - 30% - deadline end of semester
* [Final project](https://usamec.github.io/ml2/projects) aka Replication of recent paper results - 50%
  * General deadline 25th June
  * Deadline for 5th year students - around 27th May

Getting less than 50% from autograd project -> FX.
Otherwise typical grading scheme (more than 90% - A, 89% - 80% - B, ..., 59 % - 50% - E, less than 50% - FX).

## Useful links

* [Various transformer implementations](https://github.com/lucidrains/vit-pytorch)
* [Timm library for image training](https://github.com/rwightman/pytorch-image-models)
* [Huggingface - mainly NLP models](https://huggingface.co/)

## Slides about reinforcement learning

* [slides 1](https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf), [slides 2](https://people.eecs.berkeley.edu/~pabbeel/nips-tutorial-policy-optimization-Schulman-Abbeel.pdf), [slides 3](http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/slides/lec23.pdf)

## Generative modelling

* [summary blog post](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/) (also has links to previous methods)

## Machine translation

* [slides](http://web.stanford.edu/class/cs224n/slides/cs224n-2022-lecture07-nmt.pdf)
* [summary of currently obsolete phrased based MT](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1162/handouts/cs224n-lecture4-PhraseBasedMT.pdf)

## CTC and transducers
* [CTC loss explained](https://distill.pub/2017/ctc/)
* [paper about transducers](https://arxiv.org/pdf/1511.04868.pdf)

## Neural net behaviour

* [Basic initialiation](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)
* [Double descent](https://arxiv.org/pdf/1912.02292.pdf)
* [Flat minima are better](https://arxiv.org/pdf/1609.04836.pdf)
* [Large minibatch might overfit](https://arxiv.org/pdf/1706.02677.pdf)
* [Sharpness aware minimization](https://arxiv.org/pdf/2010.01412.pdf), [improved](https://arxiv.org/abs/2203.02714)
* [Shattered gradients](http://proceedings.mlr.press/v70/balduzzi17b/balduzzi17b.pdf)
* [ResNet are just ensembles](https://proceedings.neurips.cc/paper/2016/file/37bc2f75bf1bcfe8450a1a41c200364c-Paper.pdf)
* [Visualization of landscape](https://proceedings.neurips.cc/paper/2018/file/a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf)

## Training dynamics

[Very good course](https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2021/)
