## Schedule

* Tuesday 9:50-11:20 I-23
* Wednesday 13:10-14:40 M-V

## Contact

`<surname>@fmph.uniba.sk`

## Grading

Three projects:
* [Autograd](https://usamec.github.com/ml2/hw1) - 20% - deadline 9th March 
* Simple project (like RNN) - 30%
* Replication of recent paper results - 50%

Getting less than 50% from autograd project -> FX.
Otherwise typical grading scheme (more than 90% - A, 89% - 80% - B, ..., 59 % - 50% - E, less than 50% - FX).

## Recommended prerequisites

* Either Machine learning or Neural networks course (ideally both)
* No fear of math (gradients)
* Reasonable coding skills in Python

## Schedule

### 20.2.

### 21.2.

### 27.2

### 28.2

* Details from [Understanding the difficulty of training deep feedforward neural networks
](http://proceedings.mlr.press/v9/glorot10a.html)
* Tricks for deep architectures:
  * Good initialization (see paper above)
  * Batch norm (see last lesson)
  * Relu units
  * Residual networks (just google it)
* Talking about vanishing and exploding gradients [On the difficulty of training recurrent neural networks](http://proceedings.mlr.press/v28/pascanu13.pdf)
* [LSTM](http://web.eecs.utk.edu/~itamar/courses/ECE-692/Bobby_paper1.pdf), [GRU](https://arxiv.org/pdf/1412.3555.pdf), [IRNN](https://arxiv.org/pdf/1504.00941.pdf)
* [Highway networks](https://arxiv.org/pdf/1505.00387.pdf)
